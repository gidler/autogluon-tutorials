{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gidler/autogluon-tutorials/blob/main/tutorials/multimodal/clip_embedding.ipynb)\n",
    "[![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/gidler/autogluon-tutorials/blob/main/tutorials/multimodal/clip_embedding.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP in AutoMM - Extract Embeddings \n",
    "\n",
    "We have shown CLIP's amazing capability in performing zero-shot image classification in our previous tutorial :ref:`sec_automm_clip_zeroshot_imgcls`. Thanks to the contrastive loss objective and trained on millions of image-text pairs, CLIP learns good embeddings for both vision and language, and their connections. Hence, another important use case of CLIP is to extract embeddings for retrieval, matching, ranking kind of tasks.\n",
    "\n",
    "In this tutorial, we will show you how to use AutoGluon to extract embeddings from CLIP, and then use it for a retrieval problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Embeddings\n",
    "\n",
    "CLIP has a two-tower architecture, which means it has two encoders: one for image, the other for text. An overview of CLIP model can be seen in the diagram below. Left shows its pre-training stage, and Right shows its zero-shot predicton stage. By computing the cosine similarity scores between one image embedding and all the text images, we pick the text which has the highest similarity as the prediction.\n",
    "\n",
    "![CLIP](https://github.com/openai/CLIP/raw/main/CLIP.png)\n",
    "\n",
    "Given the two encoders, we can extract image embeddings, or text embeddings. And most importantly, embedding extraction can be done offline, only similarity computation needs to be done online. So this means good scalability. \n",
    "\n",
    "First, let's download some images. These images are from [COCO datasets](https://cocodataset.org/#home)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal import download\n",
    "urls = ['http://farm4.staticflickr.com/3179/2872917634_f41e6987a8_z.jpg',\n",
    "        'http://farm4.staticflickr.com/3629/3608371042_75f9618851_z.jpg',\n",
    "        'https://farm4.staticflickr.com/3795/9591251800_9c9727e178_z.jpg',\n",
    "        'http://farm8.staticflickr.com/7188/6848765123_252bfca33d_z.jpg',\n",
    "        'https://farm6.staticflickr.com/5251/5548123650_1a69ce1e34_z.jpg']\n",
    "image_paths = [download(url) for url in urls]\n",
    "print(image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract some image embedding from the CLIP vision encoder,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor\n",
    "predictor = MultiModalPredictor(hyperparameters={\"model.names\": [\"clip\"]}, problem_type=\"zero_shot\")\n",
    "# extract image embeddings.\n",
    "image_embeddings = predictor.extract_embedding({\"image\": image_paths})\n",
    "print(image_embeddings['image'].shape)  # image (5, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output has a shape of (5, 768), because there are 5 images, each of the image embedding has a dimension of 768. \n",
    "\n",
    "Similarly, you can also extract text embeddings from the CLIP text encoder,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text embeddings.\n",
    "text_embeddings = predictor.extract_embedding({\"text\": ['There is a carriage in the image']})\n",
    "print(text_embeddings['text'].shape)  # text (1, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can use the embeddings for a range of tasks such as image retrieval, text retrieval, image-text retrieval and matching/ranking. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image retrieval by text query\n",
    "\n",
    "Suppose we have a large image database (e.g., video footage), now we want to retrieve some images defined by a text query. How can we do this? \n",
    "\n",
    "It is simple. First, extract all the image embeddings either online or offline as shown above. Then, compute the text embedding of the text query. Finally, compute the cosine similarity between the text embedding and all the image embeddings and return the top candidates. \n",
    "\n",
    "Let's reuse the example we have above. We already have 5 image embeddings in **image_embeddings**, and 1 text embedding in **text_embeddings**, now we normalize the feature and then compute their similarities,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = image_embeddings['image']\n",
    "text_features = text_embeddings['text']\n",
    "import numpy as np\n",
    "similarity = np.matmul(image_features, text_features.T)\n",
    "print(np.argmax(similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we successfully find the image with a carriage in it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "pil_img = Image(filename=image_paths[2])\n",
    "display(pil_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to switch to another text query, we simply re-compute text embeddings and do this similarity comparison again,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = predictor.extract_embedding({\"text\": ['There is an airplane over a car.']})\n",
    "text_features = text_embeddings['text']\n",
    "similarity = np.matmul(image_features, text_features.T)\n",
    "print(np.argmax(similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we find the image most corresponding to the text query. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_img = Image(filename=image_paths[4])\n",
    "display(pil_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Examples\n",
    "\n",
    "You may go to [AutoMM Examples](https://github.com/awslabs/autogluon/tree/master/examples/automm) to explore other examples about AutoMM.\n",
    "\n",
    "## Customization\n",
    "To learn how to custom AutoMM, please refer to [tutorials/multimodal/customization.ipynb](https://github.com/gidler/autogluon-tutorials/blob/main/tutorials/multimodal/customization.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('ag_dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0653dfff7333573d9fbf16a74134d166c986921740aafebc8debea4285acbf6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
